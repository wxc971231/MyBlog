- 文章链接：[Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions](https://aclanthology.org/2024.eacl-long.16/)
- 发表：EACL 2024
- 代码：未提供
- 领域：LLM-Clarification
- 一句话总结：在协作对话式绘图任务中，传统对话系统往往要么始终不提问直接执行（易在关键信息缺失时累积错误），要么模仿人类/用外部决策器学习 “何时提问”（但人类澄清策略多样、与模型不确定性相关性弱，导致提问时机不稳定）；本文提出基于 Silent Drawer 的**不确定性驱动澄清框架** QDrawer，**先对下一步动作的预测分量显式估计不确定性（用分布熵等度量），再在不确定性超过阈值时以模板化澄清问题主动补齐缺失信息并将问答反馈到后续决策**，从而在可控的提问次数下显著提升预测准确率与整体任务相似度
- ----
- 摘要：澄清问题是对话中一种关键工具，用于表明误解、歧义以及语言使用中的欠具体化。尽管人类从儿童时期起就能够通过提问来消解不确定性，**现代对话系统却难以生成有效的问题**。为推动该方向的发展，我们以一个协作式对话任务作为试验平台，研究模型不确定性与人类不确定性之间的关系——这是一个迄今研究不足的问题。我们表明，**模型不确定性并不会映射人类的澄清寻求行为，这意味着将人类澄清问题作为监督来决定何时提问，可能并不是消解模型不确定性的最有效方式**。为解决这一问题，我们提出一种基于模型不确定性估计来生成澄清问题的方法，并将其与若干替代方案进行比较，结果显示该方法在任务成功方面带来了显著提升。我们的发现强调：为对话系统赋予评估自身不确定性并在交互中加以利用的能力具有重要意义

@[toc]
# 1. 背景
## 1.1 LLM 的澄清提问
- 在日常对话中，人类可以几乎毫不费力地通过澄清性问题来处理并解决不确定性，但 LLM 并不天然具备这种能力，早期的先进 LLM 在面对含糊指令时也难以生成澄清性问题
- LLM 流行之前，**对话系统**领域已经意识到澄清性提问的重要性，当时的主流方法通常是基于规则的
	- **`基于规则的方法`：在对话管理层按规则（如置信度阈值）或决策论（如最大化期望效用）触发，再用提问模板或少量规则把 “要澄清的点” 表面化成问题**
- 近年来在 **LLM 协作式对话任务**背景下，人们对这一目标的兴趣再度升温，基本方法可以分成两类
	- **`深度学习类方法`：这类方法通过构造人类澄清提问数据集训练深度学习模型，使其能够决定提问时机**。但由于不同的人对确认/求证有不同偏好（马上确认、先推断再提问、委婉地暗示等），构造数据集往往含有混合的澄清模式。对于监督学习这会导致标签冲突，对于强化学习这会放大奖励噪声，导致模型性能受到影响
	- **`基于不确定性的方法`：这类方式使用模型的内部不确定性决定 “何时问” 或 “问什么”**，不确定度量包括 “动作集合上模型预测分布的熵”、“ASR置信分数” 等；还有些工作不显示计算不确定度，而是先基于 beam search 生成多个候选输出，然后用 “top-2 概率差” 来近似不确定性，或使用中间预测引导输出重排序。这些工作可以取得较好的性能，但本文之前**没有工作深入分析过模型不确定性和人类不确定性的相关程度**
## 1.2 CoDraw 任务
- 本文以 CoDraw 任务与数据集作为测试平台，研究协作式智能体中由**不确定性驱动的澄清策略**。CoDraw 是一个协作绘画游戏，涉及在**部分可观测**的虚拟环境中进行感知、交流和行动
	![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b9d3603ef78c40699279c1db97515d92.png#pic_center =60%x)
	 如图所示，游戏由两名玩家 Teller 和 Drawer 交互进行。Teller 看到由剪贴画元素组成的抽象场景，其布局具有语义意义；Drawer 看到的是一个初始为空白的画布。两名玩家需要通过自然语言交流进行协作，使绘制者用剪贴画重构讲述者看到的图像
		![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fe4644f325d246af994a6693d969fc87.png#pic_center =90%x)
- Drawer 模型的性能由目标图像和 Drawer 在对话结束时绘制图像的相似度来度量。相似度分数是多方面质量的加权分数：包括检查目标场景中的剪贴画是否出现在重建场景中、绘制剪贴画的属性（大小、位置、翻转、人物面部表情与身体姿态等附加属性）等。本文作者在此基础上对相似度分数指标做了一些修改
- [CoDraw](https://github.com/facebookresearch/CoDraw) 任务包含一个带有细粒度标注澄清请求的数据集，作者使用这些标注在研究了人类不确定性与模型不确定性之间的关系，并提出一个 CoDraw Drawer 模型，使其基于自身不确定性提出澄清问题

# 2. 本文方法
## 2.1 CoDraw 不确定性分析 
- CoDraw 中 Drawer 模型收到一条指令后需要就 “绘制哪些剪贴画” 以及 “它们对应的属性” 作出决策。作者想设计一个基于内部不确定性进行提问的模型，因此设计了基于 Silent Drawer 模型的 CoDraw 绘制决策不确定性估计方法
### 2.1.1 Silent Drawer 模型
- Silent Drawer 模型简单地把 Teller 的描述转换为绘制决策，不输出任何自然语言进行对话，因此其被称为 “Silent”，该模型使用 CoDraw 数据集训练，平均相似度分数为 3.31
	- 模型输入：使用双向 LSTM 提取 Teller 描述的特征，用一个向量描述当前画布状态（已添加的剪贴画及其属性），二者拼接后作为输入
	- 中间层：稠密的前馈神经网络
	- 模型输出：一个大向量，包含 58 个可用剪贴画的大小、位置、朝向和一个无界分数，分数高于 0 的剪贴画都会被绘制到画布上
- Silent Drawer 的训练损失由多个部分组成
	- 剪贴画大小是离散的，对 small、medium、large 做交叉熵损失
	- 剪贴画朝向是离散的，对 right、left  做交叉熵损失
	- 剪贴画位置是连续的，对 x, y 做 L2 损失
	- 剪贴画分数是无界实数，其先通过 sigmoid 变为概率形式，再用二分类交叉熵训练
### 2.1.2 CoDraw 决策的模型不确定性度量
- 基于 Silent Drawer 模型，作者设计的 CoDraw 决策不确定性度量如下
	1. 对于 **剪贴画选择**，使用 Silent Drawer 分配给每个剪贴画的分数作为模型不确定性的度量。分数越低越不确定
	2. 对于 **大小（small、medium、large）** 与 **朝向（right、left）** 属性，使用每个属性在可能标签上的概率分布的熵描述该标签上的不确定性
	3. 对于 **位置** 属性，Drawer 为每个剪贴画输出两个值，对应画布的 x 轴与 y 轴坐标。由于度量连续值的不确定性比离散变量更困难，作者使用模型集成方法，将**使用不同随机初始化种子训练得到的五个 Silent Drawer 模型的预测方差**作为位置不确定性度量

## 2.2 模型不确定性与人类不确定性相关性低
- 作者分析了**模型内部的不确定性是否和人类玩家相似**。为此，使用针对澄清提问进行了精细标记的 CoDraw-iCR 数据集，其中约 40% 的对话至少出现一次澄清请求，用这些澄清请求作为人类不确定的信号。**作者使用探针思想，以模型不确定性为输入，训练逻辑回归模型预测人类是否会发起澄清请求，若预测性能良好，则认为二者有高度一致性**。具体地
	- 使用 2.1.2 节所述方法计算每轮的模型不确定性，不确定性是一个4维向量，描述了剪贴画选择、大小、朝向、位置的不确定性（若一轮涉及多个剪贴画绘制动作，则取各项不确定性的最大值
	- 逻辑回归模型输入模型不确定性4维向量，预测 “下一回合人类是否会提出澄清问题”
- 实验表明
	1. 逻辑回归的平均精度仅 0.188（略高于随机 0.142），且只有 clipart 选择不确定性具显著性，**说明模型不确定性与人类不确定性相关性总体较弱**
	2. 进一步分析发现人类澄清策略高度不一致：即便指令遗漏关键属性（如未给 size 或未给位置），人类也常常不会追问，且追问时也不一定针对缺失属性。因此作者认为**用人类澄清决策来监督学习 “何时提问” 可能并非最优**
		> 人类可能更谨慎或更冒险，承担更多或更少风险，或猜测/希望之后的对话中会补充更多信息，从而导致在 “是否澄清” 决策上的高度分歧

## 2.3 基于不确定性提出澄清问题
- 基于 2.2 分析，使用带澄清提问的人类数据微调 LLM 很可能不是好思路，因此作者聚焦于使用模型内部的不确定性来触发澄清提问。具体地，**仅针对剪贴画的尺寸属性进行提问**，因为该属性对于所有剪贴画是普适的，且人类 Teller 常常不会指明尺寸。目标是**验证由尺寸不确定性触发的提问能否提升 Drawer 在尺寸属性的准确率和整体相似度分数**
- 作者对 2.1.1 节所述的 Silent Drawer 进行改造得到 QDrawer，**其模型结构完全不变，但修改了交互方式，使其可以基于尺寸不确定性发起澄清提问**。为了隔离 “提问” 本身的影响、并避免与语言生成相关的混杂因素，作者采用 **基于模板（template-based）的方法来生成澄清问题及其回答**
	> 这里所谓 “基于模板” 其实就是一些填词规则，使用这种方式的原因，除了避免语言生成影响以外，Silent Drawer 和 QDrawer 这种模型结构本身也不支持生成自然语言
- QDrawer 和 Silent Drawer 的主要区别在于交互方式不同，具体而言
	- Silent Drawer 每轮交互输入场景描述和画布状态 $(\text{Teller}_t, \text{Canvas}_t)$，输出绘制动作 $a_t$，即 $$\begin{aligned}a_t &= \pi(\text{Teller}_t, \text{Canvas}_t) \\ \operatorname{Canvas}_{t+1}&=\operatorname{Apply}\left(\operatorname{Canvas}_{t}, a_{t}\right) \end{aligned}$$ 其中 $\text{Teller}_t$ 是数据集中的原始描述，遍历完数据样本的全部 $N$ 条描述即结束
		> - 训练阶段，CoDrawer 包含 Drawer 的澄清提问，但是相关内容被忽略
		> - 推理阶段，根据 CoDraw 的任务机制要求，Drawer 每轮必须回复 Teller，由于 Silent Drawer 没有语言能力，其默认输出一个确认性占位回答（如 OK）
	- QDrawer 外层也用 $\text{Teller}_t$ 遍历数据样本的全部 $N$ 条描述，但第 $t$ 个绘制动作 $a_t$ 生成前可能触发内层澄清问答，如下图所示
		![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/82f2774d5afd4f36910e378205bdcb33.png#pic_center =98%x)
		1. Drawer 输入上一轮对话，生成候选动作
			$$
			\hat{a}_t = \pi(\text{Drawer}_{t-1}, \text{Teller}_t, \text{Canvas}_t)
			$$ 这里 $\text{Drawer}_{t-1}$ 是上一轮 Drawer 的回复
			- 当 $\text{Drawer}_{t-1}$ 为确认性占位符时，$\text{Teller}_t$ 是样本中下一条场景描述
			- 当 $\text{Drawer}_{t-1}$ 为澄清性提问时，$\text{Teller}_t$ 是问题解答
		2. 检查 $\hat{a}_t$ 中的剪贴画分数，**分数超过阈值的剪贴画成为候选，需要被绘制或更新**，计算所有候选剪贴画 size 属性的熵 $H_{\text{size}}(\hat{a}_t)$ 并从大到小排序
		3. 若候选剪贴画的最大熵不确定性未超过阈值 $\theta$，则用 $\hat{a}_t$ 作为本轮最终动作，并生成确认性占位回答（OK）。否则 Drawer 取不确定度 top2 剪贴画对象，针对其中不确定度超过阈值的剪贴画 **使用模板生成问题** $\text{q}_t$，Teller **用真值模板生成回答** $\text{ans}_t$，然后把这段问答作为条件再次生成最终动作，并更新画布
			$$
			\begin{aligned}
			&a_t = 
			\begin{cases}
			\hat{a}_t, &H_{\text{size}}(\hat{a}_t) \leq \theta \\
			\pi(\text{Drawer}_{t-1}, \text{Teller}_t, \text{q}_t, \text{ans}_t, \text{Canvas}_t), &\text{otherwise} 
			\end{cases} \\ 
			&\space \\
			&\operatorname{Canvas}_{t+1}=\operatorname{Apply}\left(\operatorname{Canvas}_{t}, a_{t}\right)
			\end{aligned}
			$$ 
			> 问题 $\text{q}_t$ 是通过模版生成的，一次最多对两个剪贴画对象进行提问，模板为 
			> - “what size is the __?”
			> - “what size are the __ and the __?”
			> 
			> 回答 $\text{ans}_t$ 也是使用数据集样本真值填充模板得到，模板为
			> - “The __ is __” 
			> - “The __ is __ and the __ is __” 
			> 
			> 注意 QDrawer 推理时可能带问答交互也可能不带，序列长度的变化依赖 LSTM 特征提取器实现兼容
- 为了实现以上新的交互逻辑，作者对模型的训练方式做了调整，为了保持训练推理一致性，QDrawer 对每一轮 “修改” 和 “新增” 的剪贴画都计算 loss 并进行优化。具体地， CoDraw 数据里每轮都有对应的**人类绘制动作/画布更新结果**，在训练模型之前，**通过对比相邻两帧画布，就能差分得到 “新增” 和 “修改” 的剪贴画**，然后在训练时
	- Silent Drawer 只对 “新增” 部分计算 CE loss；
	- QDrawer 对 “新增” 和 “修改” 部分都计算 CE loss
		> 这里关键是**把 “修改” 的剪贴画也作为分数 BCE loss 的正例**，从而让这些剪贴画的分数上升
- 综上，QDrawer 方法的框架图如下
	![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c735b63a06674019abe71ff92a453c54.png#pic_center =90%x)

# 3. 实验
## 3.1 实验设置
- 作者使用 CoDraw test set 数据集进行实验，对比基线包括
	1. **SD**：即 Silent Drawer，不提问
	2. **QD-Human**：在 “人类玩家问 size 问题” 的相同回合提问
	3. **QD-$\theta$**：不确定性驱动的 QDrawer，阈值 $\theta\in\{0.3,0.7,1.1\}$ 控制何时触发提问
	4. **QD-Decider**：用带有人类提问标注的数据训练一个独立的 Decider 模型决定何触发时问。注意这里人类澄清提问不限于 size 属性
- 评价指标
	1. **Size accuracy（SA）**：画布上所有已绘制剪贴画的 size 预测准确率（%）
	2. **Similarity score（SS）**：最终绘制场景与目标场景的整体相似度分数
	3. **CQ SA boost**：取至少提出过一次澄清问题的子数据集 `CQ`，然后将这些对话中的澄清问题全部移除，得到数据集 `!CQ`，使用模型分别对二者进行推理绘图，计算 Size accuracy 的差值
	4. **CQ SS boost**：取至少提出过一次澄清问题的子数据集 `CQ`，然后将这些对话中的澄清问题全部移除，得到数据集 `!CQ`，使用模型分别对二者进行推理绘图，计算 Similarity score 的差值
## 3.2 实验结果
- 主实验结果如下，可见**基于内部不确定性发起提问的 QD-$\theta$ 实现了最高准确率，且澄清交流带来的增益显著高于基线模型**
	![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/2adca76863ae43b2a79cb54318071c52.png#pic_center =60%x)
- 作者进一步考虑了提出澄清问题的成本。首先注意到**可以用不确定度阈值控制提问数量**，从而间接地控制提问成本
	![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/88501a14bf074fab852701d7290d8843.png#pic_center =60%x)
	作者对不同提问数量（成本）对 size 精度的影响进行分析，发现**基于内部不确定性发起提问的 QD-$\theta$ 方法始终具有最高的性能提升效率**，无论进行几次澄清提问，始终具有优势。而**由人类数据学习到提问时机的性能和随机提问性能类似**
	![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f031cd44a4e74ebb941def2972bb0da5.png#pic_center =60%x)
	> 对比方法包括
	> 1. **QD(Random)**：在随机对话轮次提问、但控制不同的 “每段对话平均提问数”；
	> 2. **QD(Decider)**：使用人类数据训练的决策模型触发提问，通过忽略部分触发控制  “每段对话平均提问数”。这种方式只能在 Decider 的平均提问数基础上减小，其最大值由训练好的 Decider 决定，为 1.5
	
# 4. 总结
- 本文在 CoDraw 这个问答任务上对比测试了 “模仿人类提问时机” 和 “基于内部不确定性” 两种发出澄清性提问的方法，发现**基于内部不确定性的提问触发方法具有更高的有效性和计算效率**，相比而言，**由于人类发起澄清提问的时机本身就具有高度异质性，基于其学到的提问时机并非最优**
- 虽然作者的观测具有启发性，但本文的实验设置过于简化，难以保证该结论能直接推广到更复杂的协作任务与模型架构中