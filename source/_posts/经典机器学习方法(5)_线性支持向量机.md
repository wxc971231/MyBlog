---
title: 经典机器学习方法(5)——线性支持向量机
date: 2025-09-08 09:42:35
index_img: /img/经典机器学习方法(5)_线性支持向量机/img_009.png
tags:
  - 机器学习
  - SVM
  - sklearn
categories:
  - 机器学习
  - 实践
description: 本文介绍支持向量机(SVM)的硬间隔最大化和软间隔最大化原理,包括线性可分SVM与线性SVM的优化问题建模、对偶算法及Python实现。
---

- 首发链接：[经典机器学习方法(5)——线性支持向量机](https://blog.csdn.net/wxc971231/article/details/127134602)
- 参考：
    1. 《统计学习方法（第二版）》第 7 章
    2. 白板机器学习
- `支持向量机器(support vector machines, SVM)` 可以看作之前介绍过的 [经典机器学习方法（4）—— 感知机](https://blog.csdn.net/wxc971231/article/details/126512982) 的升级版，它们都是**用于处理二分类问题的判别模型**，SVM 进一步解决了感知机只能处理线性可分样本数据、结果不唯一、只能给出线性分类面等问题。**SVM 方法包含从简单到复杂的一系列模型，其中简单模型是复杂模型的特殊情况；复杂模型是简单模型的推广**，和感知机对比如下
    | 模型  | 适用问题 |数据要求 | 求解策略|得到分类超平面|
    |--|--|--|--|--|
    | 感知机 |线性二分类问题 | 线性可分 | 误分类最小化|线性，不唯一|
    | 线性可分支持向量机器 |线性二分类问题 | 线性可分 | 硬间隔最大化|线性，唯一|
    | 线性支持向量机器 | 线性二分类问题 |近似线性可分 | 软间隔最大化|线性，不唯一|
    | 非线性支持向量机器 | 非线性二分类问题 |无 | 核计巧 + 软间隔最大化|非线性，唯一|
- 需要注意的一点是，无论感知机还是 SVM 本质都是线性分类器，**它们都是先把输入空间（欧式空间或离散集合）中的样本特征映射到特征空间（欧式空间或希尔伯特空间）中，使之成为特征空间里的一个线性二分类问题进行求解**
- 受篇幅限制，本文仅讨论 “线性可分支持向量机” 和 “线性支持向量机”，它们分别要求数据线性可分和近似线性可分，这时特征空间和输入空间相同
------

# 1. 线性可分 SVM 与硬间隔最大化
- 给定一个**特征空间**上的**线性可分**训练数据集 
    $$
    T = \{(\pmb{x}_1,y_1), (\pmb{x}_2,y_2), ..., (\pmb{x}_N,y_N)\}
    $$
    其中 $\pmb{x}_i\in\mathcal{X}=\mathbb{R}^n, \space y_i\in\mathcal{Y} =\{+1,-1\},i=1,2,...,N$。学习目标是求解线性的分离超平面 $(\pmb{w}^*,b^*)$
    $$
    \pmb{w^{*\top}x} +b^*  = 0
    $$
    以及相应的分类决策函数
    $$
    f(x) = \text{sign}(\pmb{w^{*\top}}\pmb{x} +b^*)
    $$
- 可见线性可分支持向量机的输入数据和感知机是完全相同的，这时可以有无限多个超平面将两类数据正确划分，**为了能够得到唯一的最优决策函数，SVM 增加了一个要求，即 “间隔最大化”**。从直观上看，**对训练数据集找到间隔最大的超平面意味着以充分大的置信度对训练数据进行分类**，也就是不仅要将正负实例点分开，而且要对最难分的实例点（距离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力

## 1.1 函数间隔和几何间隔
- **超平面**$(\pmb{w},b)$**关于某个样本点** $(\pmb{x}_i,y_i)$ 的 **`几何间隔`** 就是几何意义上直接用点到直线距离公式 $\frac{1}{||\pmb{w}||}|\pmb{w^\top x}+b|$ 计算出的距离，在二分类视角下定义为
    $$
    \gamma_i = \frac{1}{||\pmb{w}||}y_i(\pmb{w^\top}\pmb{x}_i+b)
    $$
    对于被正确分类的样本这个间隔是正数，而误分类点的间隔是负数
- 从感知机模型引出函数间隔，[经典机器学习方法（4）—— 感知机](https://blog.csdn.net/wxc971231/article/details/126512982) 里定义的距离是上述几何间隔的相反数，这样误分类点的距离就是正数，损失就可以自然地设定为 $-\frac{1}{||\pmb{w}||}\sum_{\mathbf{x_i}\in M}y_i(\pmb{w^\top x_i}+b)$，其中 $M$ 是误分类样本集合。因为数据集是线性可分的，所以这个损失一定可以优化到零，这样就可以把系数 $\frac{1}{||\pmb{w}||}$ 去掉，最终损失为
    $$
    L(\pmb{w},b) = -\sum_{\mathbf{x_i}\in M}y_i(\pmb{w^\top x_i}+b)
    $$
    其中被求和的部分就是**超平面**$(\pmb{w},b)$**关于某个样本点** $(\pmb{x}_i,y_i)$ 的 **`函数间隔`** 了
    $$
    \hat{\gamma}_i = y_i(\pmb{w^\top}\pmb{x}_i+b)
    $$
    对于被正确分类的样本这个间隔是正数，而误分类点的间隔是负数
- 接下来可以定义**超平面**$(\pmb{w},b)$**关于训练数据集 $T$ 的`函数间隔`和`几何间隔`为**
    $$
    \begin{aligned}
    &几何间隔：\gamma = \min_{i=1,...,N} \gamma_i \\
    &函数间隔：\hat{\gamma} = \min_{i=1,...,N} \hat{\gamma}_i
    \end{aligned}
    $$
    **线性可分 SVM 的优化目标就是要在正确划分训练集的情况下最大化超平面**$(\pmb{w},b)$**关于训练数据集 $T$ 的间隔**，那么该使用函数间隔还是几何间隔呢 ？
- 观察公式可见函数间隔 $\hat{\gamma}$ 和几何间隔 $\gamma$ 间的关系就是差一个系数 $\frac{1}{||\pmb{w}||}$，即
    $$
    \gamma_i = \frac{\hat{\gamma}_i}{||\pmb{w}||} \quad \gamma= \frac{\hat{\gamma}}{||\pmb{w}||}
    $$
    这个系数对于感知机无关紧要，但对 SVM 模型来说很重要，因为
    1. 感知机只考虑 **“最小化所有误分类点到超平面的错误间隔之和”**，这在线性可分数据集下一定可以优化到零，我们只要在优化过程中定性地检查损失是否为 0 集合，系数 $\frac{1}{||\pmb{w}||}$ 并不影响这个定性指标的计算
    2. SVM要考虑 **“最大化训练数据集到超平面的最小间隔”**，这时我们就必须定量地将间隔大小计算出来。超平面 $(k\pmb{w},kb)$ 和 $(\pmb{w},b)$ 是同一个平面的两种表示方法，但在没有系数 $\frac{1}{||\pmb{w}||}$ 的情况下前者的函数间隔是后者的 $k$ 倍，这会干扰计算

    综上所述，SVM 模型定义优化问题时有两个选择
    1. 使用几何间隔 $\gamma$
        > 这是形式化 SVM 优化问题的出发点
    2. 使用函数间隔 $\hat{\gamma}$，并增加 $\hat{\gamma}$ 等于某常数的约束
         > 基于几何间隔的优化问题经过简化后，得到的最终优化问题是这个形式
## 1.2 硬间隔最大化
- SVM 学习的基本想法是求解 **`能够正确划分数据集`** 且 **`几何间隔最大`** 的分离超平面，其中第二个要求保证得到的解唯一
- 对于线性可分的训练数据集，所有样本点到最终的分离超平面的间隔一定都是正数，因此称其为 **`硬间隔最大化`**
### 1.2.1 问题建模
- 这里我们展开 1.1 节最后的分析，首先按 SVM 学习的原始想法，把硬间隔最大化建模为如下使用几何间隔的约束优化问题
    $$
    \begin{aligned}
    &\max_{\pmb{w},b} &&\gamma \\
    &\text{s.t.}  &&y_i\big(\frac{\pmb{w^\top}\pmb{x}_i}{||\pmb{w}||}+\frac{b}{||\pmb{w}||}\big) \geq \gamma, \quad i=1,2,...,N
    \end{aligned}
    $$
    考虑几何间隔和函数间隔的关系，可将上述问题改写为以下使用函数间隔的形式
    $$
    \begin{aligned}
    &\max_{\pmb{w},b} &&\frac{\hat{\gamma}}{||\pmb{w}||} \\
    &\text{s.t.}  &&y_i\big(\pmb{w^\top} \pmb{x}_i+b\big) \geq \hat{\gamma}, \quad i=1,2,...,N
    \end{aligned}
    $$
    假设 $\hat{\gamma}$ 变成之前的 $\lambda$ 倍，即 $\lambda \hat{\gamma}$，则由函数间隔定义 $\pmb{w},b$ 都变成之前的 $\lambda$ 倍，进而 $||\pmb{w}||$ 也变成之前的 $\lambda$ 倍，这时有
    $$
    \begin{aligned}
    &\max_{\pmb{w},b} &&\frac{\lambda\hat{\gamma}}{\lambda||\pmb{w}||} \\
    &\text{s.t.}  &&\lambda y_i\big(\pmb{w^\top} \pmb{x}_i+b\big) \geq \lambda\hat{\gamma}, \quad i=1,2,...,N
    \end{aligned}
    $$
    可见**常数**$\lambda$**都可以消去，约束问题没有变化**，这说明**函数间隔**$\hat{\gamma}$**的取值不影响优化问题的解**，不妨直接将其设为 1 代入，又注意到 $\max_{\pmb{w}}\frac{1}{||\pmb{w}||} = \min_{\pmb{w}}\frac{1}{2}||\pmb{w}||^2$，最终的优化问题为
    $$
    \begin{aligned}
    &\min_{\pmb{w},b} &&\frac{1}{2}||\pmb{w}||^2 \\
    &\text{s.t.}  &&y_i\big(\pmb{w^\top} \pmb{x}_i+b\big)-1 \geq 0, \quad i=1,2,...,N
    \end{aligned}
    $$
    注意到**每一个样本点对应一个不等式约束**
    > 理论上讲，这个优化问题属于凸二次规划问题
    > 1. 若约束优化问题的**目标函数**和**不等式约束**都是 **$\mathbb{R}^n$ 上连续可微的凸函数**；**等式约束都是 $\mathbb{R}^n$ 上的仿射函数**，则其是`凸优化问题`
    > 2. 若凸优化问题的**目标函数是二次函数**且**不等式约束都是仿射函数**，则其是`凸二次规划问题`，注意它是一种特殊的凸优化问题
    > 
    > 凸优化问题具有优良性质：其局部最优解就是全局最优解。另外，凸优化问题的研究较为成熟，如果一个具体问题能被归为一个凸优化问题，基本可以确定该问题是可被求解的
### 1.2.2 线性可分 SVM 最大间隔算法
- 直接解上述约束优化得到分离超平面和相应的分类决策函数，就是**线性可分支持向量机的`最大间隔算法`**
    <div align="center">
        <img src="/MyBlog/img/经典机器学习方法(5)_线性支持向量机/img_001.png" alt="在这里插入图片描述" style="width: 80%;">
    </div>

    可以证明这样得到的**线性可分数据集的最大硬间隔分离超平面**$(\pmb{w}^*,b^*)$**是存在且唯一的**
    > 具体证明过程见 《统计学习方法（第二版）》P.117

### 1.2.3 支持向量和间隔边界
- 仔细考虑我们的优化目标 “最大化训练数据集到超平面的最小间隔”，其中 “数据集到超平面的最小间隔”  是距离分类平面最近的样本点到平面的间隔，因此事实上**最后优化得到的分离超平面只由距离分离超平面最近的样本点决定，这些样本实例称为`支持向量support vector`**。从公式角度考虑，支持向量是使优化问题中不等式约束 $y_i\big(\pmb{w^\top} \pmb{x}_i+b\big)-1 \geq 0$ 等号成立的点，具体而言
    1. 正例点 $y_i=+1$，位于超平面 $H_1: \big(\pmb{w^\top} \pmb{x}+b\big)=1$
    2. 负例点 $y_i=-1$，位于超平面 $H_2: \big(\pmb{w^\top} \pmb{x}+b\big)=-1$

    `间隔边界` $H_1,H_2$ 平行且没有样本点落于其中，它们关于`分离超平面`对称，间隔边界之间的距离称为 `间隔margin`，它依赖于分离超平面的 $\pmb{w}$。在一个简单二分类任务中将上述概念全部图示出来，如下
<div align="center">
    <img src="/MyBlog/img/经典机器学习方法(5)_线性支持向量机/img_002.png" alt="在这里插入图片描述" style="width: 80%;">
</div>

- **只有支持向量决定了分离超平面的位置，间隔边界以外实例点的存在与否没有任何影响**，上图是一个十分巧合的情况，两个间隔边界上都有两个支持向量，只要一个向量和间隔边界稍稍不共面，它的存在也没有意义了。支持向量机由很少的 “重要的” 训练样本（支持向量）确定，这也是其名称的由来

## 1.3 学习的对偶算法
- 上面 1.2.1 节建模了原始优化问题，这种约束优化问题直接求解有时很困难，我们通常会将其转换为拉格朗日对偶问题求解，这样做有两个好处
    1. 拉格朗日对偶问题**一定是凸优化问题，而且往往能减少约束条件**，更容易解
    2. 可以**自然地引入核函数，从而推广到非线性分类问题**
- 关于转换对偶问题的方法和原理请参考 [一文看懂拉格朗日乘子法、KKT条件和对偶问题](https://blog.csdn.net/wxc971231/article/details/125966644)

### 1.3.1 原始问题转换为对偶问题
- 根据以下步骤转换对偶问题
    1.  先把原始问题变形为约束优化问题的标准形式
        $$
        \begin{aligned}
        &\min_{\pmb{w},b} &&\frac{1}{2}||\pmb{w}||^2 \\
        &\text{s.t.}  &&1- y_i\big(\pmb{w^\top} \pmb{x}_i+b\big) \leq 0, \quad i=1,2,...,N
        \end{aligned}
        $$
    2. 每个不等式约束引入拉格朗日乘子 $\alpha_i\geq 0$ 和拉格朗日乘子向量 $\pmb{\alpha}=[\alpha_1,\alpha_2,...,\alpha_N]^\top$，定义广义拉格朗日函数为
    $$
    \begin{aligned}
    L(\pmb{w},b,\pmb{\alpha})
    &= \frac{1}{2}||\pmb{w}||^2  +\sum_{i=1}^N \alpha_i(1- y_i\big(\pmb{w^\top} \pmb{x}_i+b\big)) \\
    &= \frac{1}{2}||\pmb{w}||^2  - \sum_{i=1}^N\alpha_i y_i\big(\pmb{w^\top} \pmb{x}_i+b\big) + \sum_{i=1}^N \alpha_i
    \end{aligned}
    $$
    这时原始问题等价于
    $$
    \min_{\pmb{w},b}\max_{\pmb{\alpha}:\alpha_i\geq 0} L(\pmb{w},b,\pmb{\alpha})
    $$
    对偶问题是
    $$
    \max_{\pmb{\alpha}:\alpha_i\geq 0}\min_{\pmb{w},b} L(\pmb{w},b,\pmb{\alpha})
    $$
    注意到这里我们已经解除了关于 $\pmb{w}$ 和 $b$ 的约束，如果不懂怎么推过来的请参考上面文章
    3. 进一步处理对偶问题，令偏导为零处理内层的 $\min_{\pmb{w},b}$
    $$ 
    \begin{aligned}
    &\nabla_{\pmb{w}}L(\pmb{w},b,\pmb{\alpha}) =\pmb{w}-\sum_{i=1}^N \alpha_i y_i x_i=0 &\Rightarrow &\quad \pmb{w}=\sum_{i=1}^N \alpha_i y_i \pmb{x}_i \\
    &\nabla_bL(\pmb{w},b,\pmb{\alpha}) =-\sum_{i=1}^N \alpha_i y_i=0 &\Rightarrow &\quad \sum_{i=1}^N \alpha_i y_i=0
    \end{aligned}
    $$
    将上面得到两个式子带回到广义拉格朗日函数得到
    $$
    \min_{\pmb{w},b}L(\pmb{w},b,\pmb{\alpha}) =-\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j\left(x_i^{\pmb{\top}} x_j\right)+\sum_{i=1}^N \alpha_i
    $$
    最后对目标取相反数，把外层的 $\max$ 变成符合习惯的 $\min$，最终得到对偶问题为
    $$
    \begin{aligned}
    &\min_{\pmb{\alpha}} && \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j\left(x_i^{\pmb{\top}} x_j\right)-\sum_{i=1}^N \alpha_i \\
    &\text { s.t. } && \sum_{i=1}^N \alpha_i y_i=0 \\
    && &\alpha_i \geqslant 0, \quad i=1,2, \cdots, N
    \end{aligned}
    $$
- 考虑原始优化问题，优化目标 $\min_{\pmb{w},b} \frac{1}{2}||\pmb{w}||^2$ 是凸函数，不等式约束 $1- y_i\big(\pmb{w^\top} x_i+b\big) \leq 0$ 是仿射函数，因此**线性可分 SVM 满足弱 slater 条件，强对偶性成立，对偶问题一定和原问题同解，进而强对偶的必要条件 KKT 条件也成立**
### 1.3.2 线性可分 SVM 对偶算法
- 考察对偶问题的最优解 $\pmb{\alpha}^*=[\alpha^*_1,\alpha^*_2,...,\alpha^*_N]^{\pmb{\top}}$，注意到 $\pmb{\alpha}^*\neq\pmb{0}$，因为
    1. 从直观上看，$\pmb{\alpha}^*=\pmb{0}$ 意味着原问题中所有不等式约束都是松弛的，而我们知道**支持向量对应的不等式约束条件一定是紧致的**，因此出现矛盾
    2. 从公式上看，$\pmb{\alpha}^*=\pmb{0}$ 意味着无论数据如何，最优分离超平面都有 $\pmb{w}^*=\pmb{0}$，这显然不合理 
- 假设对偶问题的最优解 $\pmb{\alpha}^*=[\alpha^*_1,\alpha^*_2,...,\alpha^*_N]^{\pmb{\top}}$ 中至少有一个 $\alpha_j>0$，可以原问题的最优解如下得到
    $$
    \begin{aligned}
    &\pmb{w}^*=\sum_{i=1}^N \alpha_i^* y_i \pmb{x}_i \\
    &b^* = y_j - \sum_{i=1}^N\alpha^*_iy_i(\pmb{x}_i^{\pmb{\top}} \pmb{x}_j)
    \end{aligned}
    $$
    进而得到分类决策函数
    $$
    f(\pmb{x}) = \text{sign}\Big(\sum_{i=1}^N\alpha^*_iy_i(\pmb{x}^{\pmb{\top}} \pmb{x}_j)+b^*\Big)
    $$
- 将上述过程整理为线性可分 SVM 的对偶算法
        <div align="center">
            <img src="/MyBlog/img/经典机器学习方法(5)_线性支持向量机/img_003.png" alt="在这里插入图片描述" style="width: 80%;">
        </div>

# 2. 线性 SVM 与软间隔最大化
- 线性可分支持向量机要求训练数据必须线性可分，因为线性不可分数据集不可能令 1.2.1 节原始问题中的不等式约束全部成立，这在很大程度上限制了 SVM 的实用性。数据收集过程中存在噪声，**可能数据的真实分布其实是线性可分的，但由于噪声出现了少量`特异点outlier`，这种数据集性质称为`近似线性可分`**，本节我们考虑如何将 SVM 推广应用到这种情况中
## 2.1 松弛约束
- 一个直观的想法是，允许某些样本违反 $\hat{\gamma}\geq 1$ 的约束，因此我们可以对每个样本点引入一个`松弛变量` $\xi_i\geq 0$，使得 $\hat{\gamma}+\xi_i\geq 1$，这有两种等价的理解方式
    1. 把每个样本移动一段后 $\xi_i$ 后使之满足函数间隔大于 1 的要求，即 $y_i\big(\pmb{w^\top} \pmb{x}_i+b\big) +\xi_i\geq 1$
    2. 对每个样本灵活调整间隔边界位置，使其函数间隔满足 $y_i\big(\pmb{w^\top} \pmb{x}_i+b\big) \geq 1-\xi_i$ 
- 注意，在 $\xi_i\to +\infin$ 的极端情况下，任意分离超平面都能使所有样本满足约束，这就没有意义了，所以我们**不想对模型过于宽容，每一个松弛变量**$\xi_i$**都要成比例地支付代价 $C\xi_i$**，将之体现在目标函数中
    $$
    \frac{1}{2}||\pmb{w}||^2 + C\sum_{i=1}^N \xi_i
    $$
    这里 $C>0$ 是个超参数，称为 `惩罚系数`。注意**正确分类点的**$\xi_i=0$**，因此惩罚全部来自误分类点**，$C$ 本质是在 “函数间隔尽量大” 和 “误分类点个数尽量少” 直接进行权衡
 
 ## 2.2 软间隔最大化
 ### 2.2.1 问题建模
 - 将上述讨论形式化，得到线性支持向量机优化的原始问题
     $$
     \begin{aligned}
     &\min_{\pmb{w},b,\pmb{\xi}} && \frac{1}{2}||\pmb{w}||^2 + C\sum_{i=1}^N \xi_i \\
     &\text { s.t. } &&y_i\big(\pmb{w^\top} \pmb{x}_i+b\big) \geq 1-\xi_i  \quad i=1,2, \cdots, N\\
     && &\xi_i \geqslant 0, \quad i=1,2, \cdots, N
     \end{aligned}
     $$
     **这也是一个凸二次规划问题，因而一定有解。可以证明**$\pmb{w}$**的最优解是唯一的，$b$ 的最优解则存在于一个区间**

 ### 2.2.2 支持向量和间隔边界
 - 由于引入了松弛变量，线性 SVM 的支持向量于线性可分 SVM 稍有不同，先看一个简单二分类任务，画出分离超平面 $\big(\pmb{w^\top} \pmb{x}+b\big)=0$ 及间隔边界 $\big(\pmb{w^\top} \pmb{x}+b\big)=\pm 1$ 
         <div align="center">
             <img src="/MyBlog/img/经典机器学习方法(5)_线性支持向量机/img_004.png" alt="在这里插入图片描述" style="width: 60%;">
         </div>

    上图固定了间隔边界，支持向量可以理解为 “函数间隔本身恰好为 1” 以及 “移动一段后 $\xi_i>0$ 后才能满足函数间隔 $\geq$ 1” 要求的样本。上图中标有箭头的都是支持向量，会影响优化结果。具体而言，这时支持向量可以分成三类
    1. 位于正确分类一侧的间隔边界上（和线性可分 SVM 一样），$\hat{\gamma}_i=1,\space \xi_i=0$
    2. 位于正确分类一侧的间隔边界和分离超平面之间，$0<\hat{\gamma}_i<1,\space 0<\xi_i<1$
    3. 位于分离超平面误分类一侧，$\hat{\gamma}_i\leq 0,\space \xi_i>1$
## 2.3 学习的对偶算法
- 我们必须意识到，**线性 SVM 只是线性可分 SVM 的简单扩展，从数学形式上也可以看出二者并没有本质区别**，因此和第 1 节类似，将原始优化问题转为对偶问题可以使求解更简便，而且便于后面扩展到非线性 SVM 的情况
### 2.3.1 原始问题转换为对偶问题
- 根据以下步骤转换对偶问题
    1.  先把原始问题变形为约束优化问题的标准形式
        $$
        \begin{aligned}
        &\min_{\pmb{w},b,\pmb{\xi}} && \frac{1}{2}||\pmb{w}||^2 + C\sum_{i=1}^N \xi_i \\
        &\text { s.t. } &&1-\xi_i-y_i\big(\pmb{w^\top} \pmb{x}_i+b\big) \leq 0  \quad i=1,2, \cdots, N\\
        && &-\xi_i \leq 0, \quad i=1,2, \cdots, N
        \end{aligned}
        $$
    2. 每个不等式约束引入拉格朗日乘子 $\alpha_i\geq 0, \mu_i \geq 0$ 和拉格朗日乘子向量 $\pmb{\alpha}=[\alpha_1,\alpha_2,...,\alpha_N]^\top, \pmb{\mu}=[\mu_1,\mu_2,...,\mu_N]^\top$，定义广义拉格朗日函数为
    $$
    L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu})
    =\frac{1}{2}||\pmb{w}||^2 + C\sum_{i=1}^N \xi_i -\sum_{i=1}^N \alpha_i\big(y_i(\pmb{w^\top} \pmb{x}_i+b)-1+\xi_i\big)-\sum_{i=1}^N \mu_i\xi_i \\
    $$
    这时原始问题等价于
    $$
    \min_{\pmb{w},b,\pmb{\xi}}\max_{\pmb{\alpha}:\alpha_i\geq 0} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu})
    $$
    对偶问题是
    $$
    \max_{\pmb{\alpha}:\alpha_i\geq 0}\min_{\pmb{w},b,\pmb{\xi}} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu})
    $$
    注意到这里我们已经解除了关于 $\pmb{w},b,\pmb{\xi}$ 的约束
    3. 进一步处理对偶问题，令偏导为零处理内层的 $\min_{\pmb{w},b,\pmb{\xi}}$
    $$ 
    \begin{aligned}
    &\nabla_{\pmb{w}} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) =\pmb{w}-\sum_{i=1}^N \alpha_i y_i \pmb{x}_i=0 &\Rightarrow &\quad \pmb{w}=\sum_{i=1}^N \alpha_i y_i \pmb{x}_i \\
    &\nabla_b L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) =-\sum_{i=1}^N \alpha_i y_i=0 &\Rightarrow &\quad \sum_{i=1}^N \alpha_i y_i =0  \\
    &\nabla_{\xi_i} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) =C-\alpha_i-\mu_i=0 &\Rightarrow &\quad C-\alpha_i-\mu_i=0
    \end{aligned}
    $$
    将上面得到两个式子带回到广义拉格朗日函数得到
    $$
    \min_{\pmb{w},b,\pmb{\xi}} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) =-\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j\left(\pmb{x}_i^{\pmb{\top}} \pmb{x}_j\right)+\sum_{i=1}^N \alpha_i
    $$
    最后对目标取相反数，把外层的 $\max$ 变成符合习惯的 $\min$，整理中间得到的新约束，如下
    $$
    \begin{aligned}
    &\min_{\pmb{\alpha}} &&-\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j\left(\pmb{x}_i^{\pmb{\top}} \pmb{x}_j\right)+\sum_{i=1}^N \alpha_i\\
    &\text { s.t. } && \sum_{i=1}^N \alpha_i y_i=0 \\
    && &\alpha_i \geqslant 0 \\
    && &\mu_i \geqslant 0\\
    && &C-\alpha_i-\mu_i=0, \quad i=1,2, \cdots, N
    \end{aligned}
    $$
    把利用等式约束再化简一下，得到对偶问题的最终形式
    $$
    \begin{aligned}
    &\min_{\pmb{\alpha}} &&-\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j\left(\pmb{x}_i^{\pmb{\top}} \pmb{x}_j\right)+\sum_{i=1}^N \alpha_i\\
    &\text { s.t. } && \sum_{i=1}^N \alpha_i y_i=0 \\
    && &0\leq \alpha_i \leq C, \quad i=1,2, \cdots, N
    \end{aligned}
    $$
- 容易上述验证线性 SVM 优化问题也具有强对偶性

### 2.3.2 线性 SVM 对偶算法
- 类似 1.3.2 节分析，不可能所有 $\alpha_i$ 和 $\mu_i$ 都等于 0，因此一定存在 $\pmb{\alpha}^*$ 的一个分量 $\alpha_j^*$ 满足 $0<\alpha^*_j<C$，则原始问题的解可以按下式求得
    $$
    \begin{aligned}
    &\pmb{w}^* = \sum_{i=1}^N \alpha_i^*y_i\pmb{x}_i \\
    &b^* = y_j-\sum_{i=1}^Ny_i\alpha_i^*(\pmb{x}_i^{\pmb{\top}}\pmb{x}_j)
    \end{aligned}
    $$
    进而得到分类决策函数
    $$
    f(\pmb{x}) = \text{sign}\Big(\sum_{i=1}^N\alpha^*_iy_i(\pmb{x}^{\pmb{\top}} \pmb{x}_j)+b^*\Big)
    $$
    注意**这里 $b^*$ 随着选出的 $\alpha^*_j$ 不同而不同，这是线性 SVM 优化结果不唯一的原因**
- 将上述过程整理为线性 SVM 的对偶算法
        <div align="center">
            <img src="/MyBlog/img/经典机器学习方法(5)_线性支持向量机/img_005.png" alt="在这里插入图片描述" style="width: 80%;">
        </div>

## 2.4 线性 SVM 的等价解释：优化带 $L_2$ 正则项的合页损失
- 回到 2.1 节线性 SVM 的基础想法上，我们的目标是**想要所有样本**$(\pmb{x},y)$**都满足函数间隔（确信度）大于等于 1 的约束 $y_i\big(\pmb{w^\top} \pmb{x}_i+b\big)\geq1$，如果不满足就设置成比例的惩罚**，这个思路可以形式化为**对每个样本**设置如下损失
    $$
    \begin{aligned}
    L(\pmb{x}|\pmb{w},b)
    &=\left\{
    \begin{aligned}
    &0, && y\big(\pmb{w^\top} \pmb{x}+b\big)\geq 1 \\
    &1-y\big(\pmb{w^\top} \pmb{x}+b\big), && y\big(\pmb{w^\top} \pmb{x}_i+b\big)<1
    \end{aligned}
    \right. \\
    &= [1-y\big(\pmb{w^\top} \pmb{x}+b\big)]_+
    \end{aligned}
    $$
    其中 $[·]_+$ 称为`合页损失函数(hinge loss function)`，定义为
    $$
    [z]_+
    =\left\{
    \begin{aligned}
    &z, && z \geq 0 \\
    &0, && z \leq 0
    \end{aligned}
    \right.
    $$
    重要的是，这个损失的定义和样本的松弛变量 $\xi_i$ 完全一致
    1. 如果本身函数间隔就大于等于 1，则不需要松弛，$\xi_i=0$ 
    2. 否则将样本按分离超平面法线方向移动直到函间隔为 1，$\xi_i= 1-y_i\big(\pmb{w^\top} \pmb{x}_i+b\big)$ 
- 将上述损失应用到整个训练集，并用 $\pmb{w}$ 的 $L_2$ 范数作为正则化项避免过拟合得到损失函数
    $$
    \begin{aligned}
    L(\pmb{w},b)
    &= \sum_{i=1}^N [1-y_i(\pmb{w}^{\pmb{\top}}\pmb{x}_i+b)]_+ +\lambda ||\pmb{w}||^2\\
    &= \sum_{i=1}^N \xi_i +\lambda ||\pmb{w}||^2 \\
    &= \frac{1}{C} \Big( \frac{1}{2}||\pmb{w}||^2 + C\sum_{i=1}^N \xi_i \Big) \quad\quad (取\lambda =  \frac{1}{2C})
    \end{aligned}
    $$
    回顾线性 SVM 的原问题
    $$
    \begin{aligned}
    &\min_{\pmb{w},b,\pmb{\xi}} && \frac{1}{2}||\pmb{w}||^2 + C\sum_{i=1}^N \xi_i \\
    &\text { s.t. } &&y_i\big(\pmb{w^\top} \pmb{x}_i+b\big) \geq 1-\xi_i  \quad i=1,2, \cdots, N\\
    && &\xi_i \geqslant 0, \quad i=1,2, \cdots, N
    \end{aligned}
    $$
    可见它和基于合页损失的优化问题 $\min_{\pmb{w},b} L(\pmb{w},b)$ 等价
- 最后，以横轴为函数，纵轴为损失值，把合页损失的函数曲线图画出来看一下
    <div align="center">
        <img src="/MyBlog/img/经典机器学习方法(5)_线性支持向量机/img_006.png" alt="在这里插入图片描述" style="width: 80%;">
    </div>

    1. 红色线为 0-1 损失，可以认为它是**一切二分类问题真正的损失函数**，但它不可导，无法直接使用
    2. 黑色虚线为感知机损失函数，正确分类时（函数间隔大于0）损失值为0，否则为函数间隔大小 $-y(\pmb{w}^{\pmb{\top}}\pmb{x}+b)$
    3. 蓝色线为感知机使用的合页损失函数，可见
        1. **合页损失是 0-1 损失的上界**，可以认为线性 SVM 是在优化由 0-1 损失上界构成的目标函数
        2. **合页损失比感知机损失更严格**，不仅要求分类正确，还要确信度足够高（函数间隔足够大）损失值才是0，合页损失对学习有更高的要求
    
 # 3. 代码实现
 - 使用 Python CVXOPT 凸优化库实现上述算法，为简便考虑，本节仅实现线性可分 SVM 的原始算法和对偶算法。关于该模块的使用方法可以参考 [《Python进阶系列》二十三：解决线性规划和二次型规划问题的CVXOPT模块](https://blog.csdn.net/qq_37085158/article/details/122535368)
- 使用 `sklearn.datasets` 库生成数据，关于该库的使用可以参考[【sklearn】dataset模块（2）—— 生成数据集](https://blog.csdn.net/wxc971231/article/details/116115530)，生成数据后，将其处理为 CVXOPT 库解二次规划问题的标准形式
        <div align="center">
            <img src="/MyBlog/img/经典机器学习方法(5)_线性支持向量机/img_007.png" alt="在这里插入图片描述" style="width: 80%;">
        </div>

    然后直接调库求解，进而得到原问题最优解即可

## 3.1 线性可分 SVM 的原始算法
- 完整代码如下，可以直接复制进 vscode 运行

    ```python
    import numpy as np
    from cvxopt import matrix, solvers
    from sklearn import datasets
    from matplotlib import pyplot as plt
    
    SAMPLENUM = 20
    COLORS = ['r', 'b']  
    
    def drawPlane(w1,w2,b):
        x1 = np.array([-4.0, 4.0])
        x2 = (-w1*x1-b)/w2
        plt.plot(x1, x2)
    
    # 生成数据
    my_datas = datasets.make_blobs(n_samples=SAMPLENUM, 
                                   n_features=2, 
                                   centers=np.array([[-3,-3], [3,3]]),
                                   center_box = (-10,10),
                                   cluster_std=[1.0,1.0])
    features, labels = my_datas
    labels[labels==0] = -1
    labels = labels.reshape(labels.size,1)
    
    # 用矩阵表示原二次规划问题
    P = 2*matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]])
    q = matrix([0.0, 0.0, 0.0])
    h = matrix(-1*np.ones(labels.size))
    G = np.hstack((features, np.ones((labels.size,1))))
    G = np.multiply(G, -labels)
    G = matrix(G)
    
    # 使用 cvxopt 求解
    sol=solvers.qp(P, q, G, h)
    print(sol['x'])
    w1,w2,b = sol['x']
    
    # 绘制样本，分离超平面和间隔边界
    plt.scatter(features[:, 0], features[:, 1], c=labels, s=30)
    
    drawPlane(w1,w2,b)
    drawPlane(w1,w2,b+1)
    drawPlane(w1,w2,b-1)
    
    for i in range(SAMPLENUM):
        if abs(w1*features[i][0] + w2*features[i][1] + b - 1) < 1e-3:
            plt.scatter(features[i,0], features[i,1], c='r', s=50, marker='x')
        if abs(w1*features[i][0] + w2*features[i][1] + b + 1) < 1e-3:
            plt.scatter(features[i,0], features[i,1], c='r', s=50, marker='x')
    ```
    <div align="center">
        <img src="/MyBlog/img/经典机器学习方法(5)_线性支持向量机/img_008.png" alt="在这里插入图片描述" style="width: 80%;">
    </div>

## 3.2 线性可分 SVM 的对偶算法
- 完整代码如下，可以直接复制进 vscode 运行
    ```python
    import numpy as np
    from cvxopt import matrix, solvers
    from sklearn import datasets
    from matplotlib import pyplot as plt
    
    SAMPLENUM = 20
    COLORS = ['r', 'b']  
    
    def drawPlane(w1,w2,b):
        x1 = np.array([-4.0, 4.0])
        x2 = (-w1*x1-b)/w2
        plt.plot(x1, x2)
    
    # 生成数据
    my_datas = datasets.make_blobs(n_samples=SAMPLENUM, 
                                   n_features=2, 
                                   centers=np.array([[-3,-3], [3,3]]),
                                   center_box = (-10,10),
                                   cluster_std=[1.0,1.0])
    features, labels = my_datas
    labels[labels==0] = -1
    labels = labels.reshape(labels.size,1)
    
    # 用矩阵表示对偶二次规划问题
    t = np.multiply(features, labels)
    P = np.dot(t,t.T)
    P = matrix(P)
    q = matrix(-1.0*np.ones((labels.size,1)))
    G = matrix(-1.0*np.eye(labels.size))
    h = matrix(np.zeros((labels.size,1)))
    b = matrix(0.0)
    A = matrix(labels.astype(np.double), (1,labels.size))
    
    # 使用 cvxopt 求解
    sol=solvers.qp(P, q, G, h, A, b)
    alphas = np.array(sol['x'])
    w = np.multiply(features, alphas)
    w = np.multiply(w, labels)
    
    # 还原原问题的解
    alpha_j = np.max(alphas)
    label_j = labels[np.argmax(alphas)]
    feature_j = features[np.argmax(alphas)]
    b = label_j.item() - np.sum(np.dot(w, feature_j.T), axis=0)
    w = np.sum(w, axis=0)
    w1,w2 = w[0],w[1]
    
    # 绘制样本，分离超平面和间隔边界
    plt.scatter(features[:, 0], features[:, 1], c=labels, s=30)
    
    drawPlane(w1,w2,b)
    drawPlane(w1,w2,b+1)
    drawPlane(w1,w2,b-1)
    
    for i in range(SAMPLENUM):
        if abs(w1*features[i][0] + w2*features[i][1] + b - 1) < 1e-3:
            plt.scatter(features[i,0], features[i,1], c='r', s=50, marker='x')
        if abs(w1*features[i][0] + w2*features[i][1] + b + 1) < 1e-3:
            plt.scatter(features[i,0], features[i,1], c='r', s=50, marker='x')
    ```
    <div align="center">
        <img src="/MyBlog/img/经典机器学习方法(5)_线性支持向量机/img_009.png" alt="在这里插入图片描述" style="width: 80%;">
    </div>

    考虑到 SVM 的原始问题也是一个简单的凸二次规划，这里其实看不太出来转化对偶形式的优势，但是注意到对偶形式中计算了很多向量内积，这对于引入核技巧推广到非线性 SVM 非常重要，下一篇文章进行详细介绍